# 데이터사이언스

 데이터사이언스가 필요한 이유

> 1. 대량으로 발생하고 있는 데이터
2. 빠른 발생속도, 거의 실시간
	ex) 버스운행, 날씨 등등
3. 이질적인 데이터
	ex) 비디오, 오디오,이미지, 문자 등 단순한 숫자형태 x
4. 비구조적인 데이터

EUC

>  End User Computing을 통해 원시데이터를 직접 분석할 수도 있게 되었다. 사용자가 엑셀을 사용해서, 분석, pivoting, graph 작업을 하는 예이다.

OLAP 온라인 분석처리

> OLAP은 온라인 분석처리 Online Analytical Processing의 약어로서, 데이터를 다차원으로 사전에 집계해 놓고 사용자 질의에 빠르게 대답할 수 있게 하고 있다. 매출의 예를 들어 요일별, 월별, 분기별로 시간별 합계를 별도의 실시간 처리없이 바로 볼 수 있게 하는 것이 좋은 예다. 또는 지역별, 부서별, 제품별로 구성하여 분석할 수 있다.

통계학과 빅데이터는 다르다.

> 모두 데이터를 분석한다는 공통점이 있지만, 데이터의 양에 차이가 있고, 빅데이터는 분산해서 처리해야 한다.
(통계학은 샘플링을 해서 모집단 추정)

데이터사이언스 절차

> 1. 문제를 먼저 생각하고 가설을 설정. ==> business value가 향상되는지?
2. 어떤 데이터를 수집하고 분석할지 결정
>> business value

>>경영학에서 비지니스가치는 경쟁력을 향상시키는 어떠한 유무형의 가치를 의미하는 단어. 투자수익, 사업이익 등을 예로 들 수 있다. 또한 조직이 건강하려면 관리해야 할 정량 지표를 선별하여 balanced scorecard를 만들어 추적하기도 한다.

 1) 문제정의
> 목적이 무엇인지, 문제 이해(현상이해 or 예측)
> ex) 브레인스토밍



2) 수집
> 웹사이트 크롤링, 정부 데이터 수집, API(열린광장,SNS)


3) 정제
> 데이터 구조화 어떻게?, 필요정보 추출, 분석을 위한 구조로 변


4) 저장
> 파일, DB, NoSQL


5) 분석
> 어떻게 분석?, 단순 집계 or 평균으로 충분?, 통계분석, 머신러닝, 예측이 필요한지,,,,
> 빅데이터를 분석하기 위해서는 분산 구조 필요(spark로)


6) 시각화
> 리포팅


문제 기술
> 문제 기술은 육하원칙을 생각한다.
> 
* 어떤 데이터를 대상으로,
* 어떻게 수집하고,
* 어떻게 분석할 것인지, 종속변수와 독립변수가 있는 것인지, 머신러닝으로 훈련할 것인지 등

데이터 수집
> * 웹사이트를 크롤링하거나, 정부에서 개방하는 데이터 또는 SNS에서 데이터를 수집한다. 특히 크롤링은 저작권 문제가 있어 주의가 필요하다. 수집에 필요한 OAuth 인증과 REST API를 사용해 JSON, XML 형식을 처리할 수 있다.
> * 집된 데이터는 파일 또는 NoSql을 사용하여 저장할 수 있다.

데이터 변환
> * data cleansing - 데이터에 잘못된 오류가 있으면 정제한다는 의미
* pre-processing - 데이터를 사전처리해서 머신러닝을 함. 정규화, 데이터속성 추출, 데이터 선별 등.
* data wrangling or munging - 원시데이터를 원하는 분석가능한 형식으로 변환
* ETL 수집된 데이터로부터 필요한 정보항목 또는 속성을 추출하고, 분석에 필요한 구조로 변환하여 저장하거나 모델의 입력으로 사용하는 절차를 ETL (Extract, Transform, Load)이라고 한다.
* 반면 빅데이터가 정량데이터가 아닌 경우(비구조적 데이터)가 많다.
	*	ex) 텍스트는 명사, 동사, 형용사와 같은 품사를 구분 or 단어의 유무 또는 갯수 등을 정량화하여, 단어 벡터, Word vector로 만들어 분석에 사용할 수 있다.


데이터 저장
> 수집한 데이터는 저장 후 분석하는 방식

> * 데이터베이스 (MySql같은 관계형데이터베이스)
* 파일 (바이너리 형식으로 HDFS, 하둡데이터 파일시스템)

>반면 저장하지 않고 바로 분석할 수 있는 스트리밍 방식 사용 가능

데이터 분석
>
 * 반복적 절차
	 * 분석을 하다보면 새로이 데이터가 필요하게 될 수 있고 그렇다면 앞 단계로 돌아가 데이터를 새로 수집해야 하므로 반복적으로 수행한다.

> 
* 탐색
	* 아웃라이어(Outliers), 결측값 등을 찾아 검증 및 분석이 편향되지 않도록 찾아 데이터를 전처리해야 한다.

> 
* 분석
	* Supervised ML: decision tree 등
	* Unsupervised ML: clustering 등
	* network graph
	* recommendation
	* mapreduce -> hadoop, pyspark
	* apache libcloud, scrapy

데이터시각화
> 그래프, 테이블 등을 이용하여 결과물 시각화
> 
* 지도 또는 워드클라우드를 사용하기도 한다.
* 동적으로 자바스크립트, D3 같은 라이브러리 이용 가능 
	
