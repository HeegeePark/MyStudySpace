{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스파크 기본설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "#home=os.path.expanduser(\"~\") # HOME이 설정되어 있지 않으면 expanduser('~')를 사용한다.\n",
    "#osn.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python\"\n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(os.path.expanduser(\"~\"),r\"C:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\")\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(\"spark.sql.warehouse.dir\", r\"C:\\Users\\user\\MyStudySpace\\2019-2\\BigData_Spark\\src\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List에서 RDD 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myList = [1,2,3,4,5,6,7]\n",
    "myRdd1 = spark.sparkContext.parallelize(myList)\n",
    "\n",
    "myRdd1.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일에서 RDD 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Wikipedia'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\", \"ds_spark_wiki.txt\"))\n",
    "\n",
    "myRdd2.first() # 첫 데이터만 조회하는 action 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파일에서 df 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(value=u'Wikipedia')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "myDf = spark.read.text(os.path.join(\"data\", \"ds_spark_wiki.txt\"))\n",
    "print myDf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print type(myDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- csv에서 RDD 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- take()를 하면 그 결과는 리스트가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'35, 2', u'40, 27', u'12, 38', u'15, 31', u'21, 1']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd4 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\", \"ds_spark_2cols.csv\"))\n",
    "myRdd4.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "myList=myRdd4.take(5)\n",
    "print type(myList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2차원 리스트를 만들땐 map()함수!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'35', u' 2'],\n",
       " [u'40', u' 27'],\n",
       " [u'12', u' 38'],\n",
       " [u'15', u' 31'],\n",
       " [u'21', u' 1']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd5 = myRdd4.map(lambda line: line.split(','))\n",
    "myRdd5.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celcius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c) :\n",
    "    return (float(9)/5)*c +32\n",
    "f=map(c2f,celcius)\n",
    "print f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lambda는 무명 함수이므로 함수 선언이 별도 필요 X\n",
    "- 처리 결과는 'return'을 사용하지 않아도 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x*2\n",
    "y=f(1)\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "y= lambda x:x*2\n",
    "print y(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[102.56, 97.7, 99.14, 100.03999999999999]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda c:(float(9)/5)*c +32, celcius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문자열에 map 사용해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['H'], ['e'], ['l'], ['l'], ['o'], [], ['W'], ['o'], ['r'], ['l'], ['d']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello World\"\n",
    "map(lambda x:x.split(), sentence)    # 철자가 요소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'World'], ['Good', 'Morining']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = [\"Hello World\", \"Good Morining\"]\n",
    "map(lambda x:x.split(), sentence)   # 단어가 요소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- filter() : 데이터 선별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3, 5, 13, 21, 55]\n"
     ]
    }
   ],
   "source": [
    "fib = [0,1,1,2,3,5,8,13,21,34,55]\n",
    "result = filter(lambda x: x%2, fib)\n",
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reduce() : 함수와 데이터 2개의 인자를 받고 반복문 필요 x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x가 앞의 x+y값으로 초기화(like 누적합 원리)\n",
    "reduce(lambda x, y: x+y, range(1,101))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[16] at RDD at PythonRDD.scala:48\n"
     ]
    }
   ],
   "source": [
    "nRdd = spark.sparkContext.parallelize([1,2,3,4])\n",
    "squared = nRdd.map(lambda x: x*x)\n",
    "print squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위처럼 RDD를 프린트한다고 내부가 보이지는 X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "print squared.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd100 = spark.sparkContext.parallelize(range(1,101))\n",
    "myRdd100.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단순 통계 기능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum:  10\n",
      "min:  1\n",
      "max:  4\n",
      "standard deviation: 1.118033988749895\n",
      "variance:  1.25\n"
     ]
    }
   ],
   "source": [
    "print \"sum: \",nRdd.sum()\n",
    "print \"min: \",nRdd.min()\n",
    "print \"max: \", nRdd.max()\n",
    "print \"standard deviation:\", nRdd.stdev()\n",
    "print \"variance: \", nRdd.variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many lines having 'Spark':  4\n"
     ]
    }
   ],
   "source": [
    "myRdd_spark=myRdd2.filter(lambda line : \"Spark\" in line)\n",
    "print \"How many lines having 'Spark': \", myRdd_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n"
     ]
    }
   ],
   "source": [
    "myRdd_unicode = myRdd2.filter(lambda line: u\"스파크\" in line)\n",
    "print myRdd_unicode.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- filter()를 사용해서 stopwords 제거하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['is','am','are','the','for','a', 'an', 'at']\n",
    "myRdd_stop = myRdd2.flatMap(lambda x:x.split())\\\n",
    "                    .filter(lambda x: x not in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia\n",
      "Apache\n",
      "Spark\n",
      "open\n",
      "source\n",
      "cluster\n",
      "computing\n",
      "framework.\n",
      "아파치\n",
      "스파크는\n",
      "오픈\n",
      "소스\n",
      "클러스터\n",
      "컴퓨팅\n",
      "프레임워크이다.\n",
      "Apache\n",
      "Spark\n",
      "Apache\n",
      "Spark\n",
      "Apache\n",
      "Spark\n",
      "Apache\n",
      "Spark\n",
      "아파치\n",
      "스파크\n",
      "아파치\n",
      "스파크\n",
      "아파치\n",
      "스파크\n",
      "아파치\n",
      "스파크\n",
      "Originally\n",
      "developed\n",
      "University\n",
      "of\n",
      "California,\n",
      "Berkeley's\n",
      "AMPLab,\n",
      "Spark\n",
      "codebase\n",
      "was\n",
      "later\n",
      "donated\n",
      "to\n",
      "Apache\n",
      "Software\n",
      "Foundation,\n",
      "which\n",
      "has\n",
      "maintained\n",
      "it\n",
      "since.\n",
      "Spark\n",
      "provides\n",
      "interface\n",
      "programming\n",
      "entire\n",
      "clusters\n",
      "with\n",
      "implicit\n",
      "data\n",
      "parallelism\n",
      "and\n",
      "fault-tolerance.\n"
     ]
    }
   ],
   "source": [
    "for words in myRdd_stop.collect():\n",
    "    print words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map()함수로 단어 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\", \"ds_spark_wiki.txt\"))\n",
    "sentences=myRdd2.map(lambda x:x.split(\" \"))\n",
    "sentences.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mySplit(x):\n",
    "    return x.split(\" \")\n",
    "\n",
    "sentences2=myRdd2.map(mySplit)\n",
    "sentences2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Wikipedia'],\n",
       " [u'Apache',\n",
       "  u'Spark',\n",
       "  u'is',\n",
       "  u'an',\n",
       "  u'open',\n",
       "  u'source',\n",
       "  u'cluster',\n",
       "  u'computing',\n",
       "  u'framework.'],\n",
       " [u'\\uc544\\ud30c\\uce58',\n",
       "  u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       "  u'\\uc624\\ud508',\n",
       "  u'\\uc18c\\uc2a4',\n",
       "  u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       "  u'\\ucef4\\ud4e8\\ud305',\n",
       "  u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위처럼 말고 문장을 출력하려면 반복문 사용할 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia \n",
      "-----\n",
      "Apache Spark is an open source cluster computing framework. \n",
      "-----\n",
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. \n",
      "-----\n",
      "Apache Spark Apache Spark Apache Spark Apache Spark \n",
      "-----\n",
      "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크 \n",
      "-----\n",
      "Originally developed at the University of California, Berkeley's AMPLab, \n",
      "-----\n",
      "the Spark codebase was later donated to the Apache Software Foundation, \n",
      "-----\n",
      "which has maintained it since. \n",
      "-----\n",
      "Spark provides an interface for programming entire clusters with \n",
      "-----\n",
      "implicit data parallelism and fault-tolerance. \n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for line in sentences.collect() :\n",
    "    for word in line:\n",
    "        print word,\n",
    "    print \"\\n-----\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문장의 철자 갯수 세어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"Apache Spark is an open source cluster computing framework\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 59, 32, 51, 31, 72, 71, 30, 64, 46]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.map(lambda s:len(s)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 리스트에 RDD 만들어 간단한 문자 처리 기능 (대소문자 변환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is'], ['a', 'line']]\n"
     ]
    }
   ],
   "source": [
    "myList=[\"this is\", \"a line\"]\n",
    "_rdd=spark.sparkContext.parallelize(myList)\n",
    "\n",
    "wordsRdd=_rdd.map(lambda x:x.split())\n",
    "print wordsRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is', 'AA line']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repRdd=_rdd.map(lambda x:x.replace(\"a\",\"AA\"))\n",
    "repRdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 명령어는 출력되는 결과만 바뀐 것, 실제 RDD 수정 불가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 첫글자를 대문자로 만들어서 출력하기\n",
    "- 첫째 요소 x[0]는 리스트 각 요소의 첫째 단어 'this', 'a'를 대문자로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THIS', 'A']\n"
     ]
    }
   ],
   "source": [
    "upperRDD = wordsRdd.map(lambda x: x[0].upper())\n",
    "print upperRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['THIS', 'IS'], ['A', 'LINE']]\n"
     ]
    }
   ],
   "source": [
    "upper2RDD = wordsRdd.map(lambda x: [i.upper() for i in x])\n",
    "print upper2RDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2]\n"
     ]
    }
   ],
   "source": [
    "wordsLength = wordsRdd\\\n",
    "    .map(len)\\\n",
    "    .collect()\n",
    "print wordsLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일에 쓰기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 리스트를 RDD로 만들어 로컬 파일에 쓰기\n",
    "- 파일로 쓰이려면 rdd.coalesce()를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"['THIS', 'IS']\", u\"['A', 'LINE']\"]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "_rdd=spark.sparkContext.textFile(\"data/ds_spark_wiki_out\")\n",
    "_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy\n",
    "- 데이터를 딕셔너리처럼 만들어서 키를 선책하여 사용할 수 있는 장점"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
      "아파 아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
      "-----\n",
      "im implicit data parallelism and fault-tolerance.\n",
      "-----\n",
      "th the Spark codebase was later donated to the Apache Software Foundation,\n",
      "-----\n",
      "Wi Wikipedia\n",
      "-----\n",
      "Ap Apache Spark is an open source cluster computing framework.\n",
      "Ap Apache Spark Apache Spark Apache Spark Apache Spark\n",
      "-----\n",
      "Sp Spark provides an interface for programming entire clusters with\n",
      "-----\n",
      "Or Originally developed at the University of California, Berkeley's AMPLab,\n",
      "-----\n",
      "wh which has maintained it since.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "myRdd_group=myRdd2.groupBy(lambda x:x[0:2])\n",
    "for (k,v) in myRdd_group.collect():\n",
    "    for eachValue in v:\n",
    "        print k, eachValue\n",
    "    print \"-----\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "_testList=[(\"key1\",1),(\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1)]\n",
    "_testRdd=spark.sparkContext.parallelize(_testList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[133] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupBy(lambda x:x[0].collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pair RDD\n",
    "- key,value 쌍으로 구성된 RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair RDD 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "_testList=[(\"key1\",1),(\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1)]\n",
    "_testRdd=spark.sparkContext.parallelize(_testList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['key1',\n",
       " 'key1',\n",
       " 'key1',\n",
       " 'key2',\n",
       " 'key2',\n",
       " 'key1',\n",
       " 'key2',\n",
       " 'key1',\n",
       " 'key1',\n",
       " 'key2',\n",
       " 'key2']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey, reduceByKey, mapValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key2', 5), ('key1', 6)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key2', <pyspark.resultiterable.ResultIterable at 0x6c5f4e0>),\n",
       " ('key1', <pyspark.resultiterable.ResultIterable at 0x6c5f550>)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key2', [1, 1, 1, 1, 1]), ('key1', [1, 1, 1, 1, 1, 1])]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', 2),\n",
       " ('key1', 2),\n",
       " ('key1', 2),\n",
       " ('key2', 2),\n",
       " ('key2', 2),\n",
       " ('key1', 2),\n",
       " ('key2', 2),\n",
       " ('key1', 2),\n",
       " ('key1', 2),\n",
       " ('key2', 2),\n",
       " ('key2', 2)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.mapValues(lambda x:x+1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어빈도 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'and', <pyspark.resultiterable.ResultIterable at 0x4ef59e8>),\n",
       " (u'\\uc18c\\uc2a4', <pyspark.resultiterable.ResultIterable at 0x4ef5fd0>),\n",
       " (u'is', <pyspark.resultiterable.ResultIterable at 0x6c78f98>)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'and', 1),\n",
       " (u'\\uc18c\\uc2a4', 1),\n",
       " (u'is', 1),\n",
       " (u'Wikipedia', 1),\n",
       " (u'AMPLab,', 1),\n",
       " (u'maintained', 1),\n",
       " (u'donated', 1),\n",
       " (u'\\ucef4\\ud4e8\\ud305', 1),\n",
       " (u'open', 1),\n",
       " (u'since.', 1),\n",
       " (u'for', 1),\n",
       " (u'\\ud074\\ub7ec\\uc2a4\\ud130', 1),\n",
       " (u'with', 1),\n",
       " (u'framework.', 1),\n",
       " (u'provides', 1),\n",
       " (u'Apache', 6),\n",
       " (u'Spark', 7),\n",
       " (u'was', 1),\n",
       " (u'Originally', 1),\n",
       " (u'which', 1)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(sum)\\\n",
    "    .take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'AMPLab,', 1),\n",
       " (u'Apache', 6),\n",
       " (u\"Berkeley's\", 1),\n",
       " (u'California,', 1),\n",
       " (u'Foundation,', 1),\n",
       " (u'Originally', 1),\n",
       " (u'Software', 1),\n",
       " (u'Spark', 7),\n",
       " (u'University', 1),\n",
       " (u'Wikipedia', 1)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x): return len(x)\n",
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(f)\\\n",
    "    .sortByKey(True)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'and', 1),\n",
       " (u'\\uc18c\\uc2a4', 1),\n",
       " (u'is', 1),\n",
       " (u'Wikipedia', 1),\n",
       " (u'AMPLab,', 1),\n",
       " (u'maintained', 1),\n",
       " (u'donated', 1),\n",
       " (u'\\ucef4\\ud4e8\\ud305', 1),\n",
       " (u'open', 1),\n",
       " (u'since.', 1),\n",
       " (u'for', 1),\n",
       " (u'\\ud074\\ub7ec\\uc2a4\\ud130', 1),\n",
       " (u'with', 1),\n",
       " (u'framework.', 1),\n",
       " (u'provides', 1),\n",
       " (u'Apache', 6),\n",
       " (u'Spark', 7),\n",
       " (u'was', 1),\n",
       " (u'Originally', 1),\n",
       " (u'which', 1),\n",
       " (u'fault-tolerance.', 1),\n",
       " (u'University', 1),\n",
       " (u'codebase', 1),\n",
       " (u'interface', 1),\n",
       " (u'data', 1),\n",
       " (u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.', 1),\n",
       " (u'Foundation,', 1),\n",
       " (u'\\uc624\\ud508', 1),\n",
       " (u'programming', 1),\n",
       " (u'\\uc2a4\\ud30c\\ud06c\\ub294', 1),\n",
       " (u'the', 3),\n",
       " (u'entire', 1),\n",
       " (u'has', 1),\n",
       " (u'to', 1),\n",
       " (u'later', 1),\n",
       " (u'computing', 1),\n",
       " (u'Software', 1),\n",
       " (u'developed', 1),\n",
       " (u\"Berkeley's\", 1),\n",
       " (u'it', 1),\n",
       " (u'an', 2),\n",
       " (u'cluster', 1),\n",
       " (u'implicit', 1),\n",
       " (u'at', 1),\n",
       " (u'\\uc2a4\\ud30c\\ud06c', 4),\n",
       " (u'of', 1),\n",
       " (u'clusters', 1),\n",
       " (u'parallelism', 1),\n",
       " (u'\\uc544\\ud30c\\uce58', 5),\n",
       " (u'California,', 1),\n",
       " (u'source', 1)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countByKey\n",
    "- 결과는 dictionary로 출력\n",
    "- coutByKey().items()하면 리스트로 변환\n",
    "- countByKey()는 dictionary로 병합하는 반면, reduceByKey()는 (K,V)로 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {u'AMPLab,': 1,\n",
       "             u'Apache': 6,\n",
       "             u\"Berkeley's\": 1,\n",
       "             u'California,': 1,\n",
       "             u'Foundation,': 1,\n",
       "             u'Originally': 1,\n",
       "             u'Software': 1,\n",
       "             u'Spark': 7,\n",
       "             u'University': 1,\n",
       "             u'Wikipedia': 1,\n",
       "             u'an': 2,\n",
       "             u'and': 1,\n",
       "             u'at': 1,\n",
       "             u'cluster': 1,\n",
       "             u'clusters': 1,\n",
       "             u'codebase': 1,\n",
       "             u'computing': 1,\n",
       "             u'data': 1,\n",
       "             u'developed': 1,\n",
       "             u'donated': 1,\n",
       "             u'entire': 1,\n",
       "             u'fault-tolerance.': 1,\n",
       "             u'for': 1,\n",
       "             u'framework.': 1,\n",
       "             u'has': 1,\n",
       "             u'implicit': 1,\n",
       "             u'interface': 1,\n",
       "             u'is': 1,\n",
       "             u'it': 1,\n",
       "             u'later': 1,\n",
       "             u'maintained': 1,\n",
       "             u'of': 1,\n",
       "             u'open': 1,\n",
       "             u'parallelism': 1,\n",
       "             u'programming': 1,\n",
       "             u'provides': 1,\n",
       "             u'since.': 1,\n",
       "             u'source': 1,\n",
       "             u'the': 3,\n",
       "             u'to': 1,\n",
       "             u'was': 1,\n",
       "             u'which': 1,\n",
       "             u'with': 1,\n",
       "             u'\\uc18c\\uc2a4': 1,\n",
       "             u'\\uc2a4\\ud30c\\ud06c': 4,\n",
       "             u'\\uc2a4\\ud30c\\ud06c\\ub294': 1,\n",
       "             u'\\uc544\\ud30c\\uce58': 5,\n",
       "             u'\\uc624\\ud508': 1,\n",
       "             u'\\ucef4\\ud4e8\\ud305': 1,\n",
       "             u'\\ud074\\ub7ec\\uc2a4\\ud130': 1,\n",
       "             u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.': 1})"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 빈도로 그래프\n",
    "- x축은 barh()로 개수를 나타내고 y축은 단어를 나타냄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "count = map(lambda x:x[0], _testList)\n",
    "word = map(lambda x:x[1], _testList)\n",
    "plt.barh(range(len(count)), count, color = 'grey')\n",
    "plt.yticks(range(len(count)), word)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combineByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lim', (336, 4)), ('lee', (99, 1)), ('kim', (252, 3))]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marks = spark.sparkContext.parallelize([('kim',86),('lim',87),('kim',75),\n",
    "                                      ('kim',91),('lim',78),('lim',92),\n",
    "                                      ('lim',79),('lee',99)])\n",
    "marksByKey = marks.combineByKey(lambda value: (value,1),\n",
    "                               lambda x,value: (x[0]+value, x[1]+1),\n",
    "                               lambda x,y: (x[0]+y[0], x[1]+y[1]))\n",
    "marksByKey.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', (718.0, 4)), ('F', (326.0, 2))]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heights = spark.sparkContext.parallelize([\n",
    "        ('M',182.),('F',164.),('M',180.),('M',185.),('M',171.),('F',162.)\n",
    "    ])\n",
    "heightsByKey = heights.combineByKey(lambda value: (value,1),\n",
    "                                   lambda x,value: (x[0]+value, x[1]+1),\n",
    "                                   lambda x,y: (x[0]+y[0], x[1]+y[1]))\n",
    "heightsByKey.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', 179.5), ('F', 163.0)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgByKey = heightsByKey.map(lambda (x,(sum,count)):(x,float(sum)/count))\n",
    "\n",
    "avgByKey.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key2', (5, 5)), ('key1', (6, 6))]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.combineByKey(lambda value: (value,1),\n",
    "                     lambda x,value: (x[0]+value, x[1]+1),\n",
    "                     lambda x,y: (x[0]+y[0], x[1]+y[1])) \\\n",
    "        .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key2', (5, 5)), ('key1', (6, 6))]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.combineByKey(lambda value: (value,1),\n",
    "                     lambda hap,count: (hap[0]+count, hap[1]+1),\n",
    "                     lambda x,y: (x[0]+y[0], x[1]+y[1])) \\\n",
    "        .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 평균 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key1': 1.0, 'key2': 1.0}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testCbkRdd=_testRdd.combineByKey(lambda value: (value,1),\n",
    "                                 lambda x,value: (x[0]+value, x[1]+1),\n",
    "                                 lambda x,y: (x[0]+y[0], x[1]+y[1]))\n",
    "\n",
    "averageByKey = _testCbkRdd.map(lambda (key, (sum,count)): (key,float(sum)/count))\n",
    "averageByKey.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제) 성적 합계 및 평균"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "marks=[\n",
    "    \"'김하나','English', 100\",\n",
    "    \"'김하나','Math', 80\",\n",
    "    \"'임하나','English', 70\",\n",
    "    \"'임하나','Math', 100\",\n",
    "    \"'김갑돌','English', 82.3\",\n",
    "    \"'김갑돌','Math', 98.5\"\n",
    "]\n",
    "_marksRdd=spark.sparkContext.parallelize(marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 3-1\n",
    "개인별 데이터를 컴마로 분리하고, 이름 x[0]과 성적 x[2]만 꺼내어 reduceBykey()를 구하면 합계를 구할 수 있다. 여기서 중요한 것은 기존 데이터에서 필요한 이름, 성적만을 꺼내어 처리한다는 점이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'임하나' 170.0\n",
      "'김하나' 180.0\n",
      "'김갑돌' 180.8\n"
     ]
    }
   ],
   "source": [
    "_marksbyname =_marksRdd\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x:(x[0], float(x[2])))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .collect()\n",
    "for i in _marksbyname:\n",
    "    print i[0],i[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제  3-2\n",
    "개인별 데이터를 컴마로 분리하고, 과목 x[1]과 성적 x[2]만 꺼내어 reduceBykey()를 구하면 합계를 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'English' 252.3\n",
      "'Math' 278.5\n"
     ]
    }
   ],
   "source": [
    "_marksbysubject =_marksRdd\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x:(x[1],float(x[2])))\\\n",
    "    .reduceByKey(lambda x,y: x+y)\\\n",
    "    .collect()\n",
    "for i in _marksbysubject:\n",
    "    print i[0],i[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 3-3\n",
    "합계, 개수를 계산해 보자. combineByKey()를 이용해서 계산해야 한다. 먼저 데이터를 이름, 과목, 데이터 -> 이름, 점수로 변경한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "_marksbyname2=_marksRdd\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x:(x[0], float(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_counts=_marksbyname2.combineByKey(\n",
    "    (lambda x: (x,1)),\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)),\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'임하나' (170.0, 2) '김하나' (180.0, 2) '김갑돌' (180.8, 2)\n"
     ]
    }
   ],
   "source": [
    "for i in sum_counts.collect():\n",
    "    for each in i:\n",
    "        print each,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문제 3-4\n",
    "개인별 평균은 3-3에서 구했던 합계, 개수를 사용하여 계산한다. 평균을 계산하기 위해 float() 형변환을 해주었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'임하나' 85.0\n",
      "'김하나' 90.0\n",
      "'김갑돌' 90.4\n"
     ]
    }
   ],
   "source": [
    "# average\n",
    "averageByKey = sum_counts\\\n",
    "    .map(lambda (key,(sum,count)):(key,float(sum)/count))\\\n",
    "    .collect()\n",
    "for i in averageByKey:\n",
    "    for j in i:\n",
    "        print j,\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 S-4: 서울시 지하철호선별 승차인원 평균 구하기.\n",
    "### 문제\n",
    "정량데이터는 보통 집단화하여 빈도, 평균, 합계 등 서술통계를 계산한다. 서울시 지하철호선별 역별 승하차 인원 정보를 가져와 평균을 구해보자.\n",
    "\n",
    "    - 파일 명 CARD_SUBWAY_MONTH_201501.csv를 다운로드 받아서 일부만 테스트용 데이터로 사용한다.\n",
    "    - 오픈API 샘플URL http://openapi.seoul.go.kr:8088/(인증키)/xml/CardSubwayStatsNew/1/5/20151101\n",
    "### 해결\n",
    "PairRDD를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sub=[\"20150101,2호선,0236,영등포구청,6199,6219\",\n",
    "\"20150101,2호선,0237,당산,7982,8946\",\n",
    "\"20150101,2호선,0238,합정,17406,15241\",\n",
    "\"20150101,3호선,0309,지축,515,538\",\n",
    "\"20150101,3호선,0310,구파발,6879,6260\",\n",
    "\"20150101,3호선,0311,연신내,20031,19470\",\n",
    "\"20150101,3호선,0312,불광,9519,11029\",\n",
    "\"20150101,4호선,0425,회현,7465,7574\",\n",
    "\"20150101,4호선,0426,서울역,3943,10823\",\n",
    "\"20150101,경부선,1002,남영,4340,4535\",\n",
    "\"20150101,경부선,1003,용산,28980,27684\",\n",
    "\"20150101,경부선,1004,노량진,23021,23862\",\n",
    "\"20150101,경부선,1005,대방,6360,6476\",\n",
    "\"20150101,경부선,1006,영등포,37247,36102\",\n",
    "\"20150101,경원선,1008,이촌,1940,1507\",\n",
    "\"20150101,경원선,1009,서빙고,911,1000\",\n",
    "\"20150101,경원선,1010,한남,1885,1863\",\n",
    "\"20150101,경원선,1011,옥수,43,37\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "_subRdd = spark.sparkContext.parallelize(_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20150101 2호선 0236 영등포구청 6199 6219\n",
      "20150101 2호선 0237 당산 7982 8946\n",
      "20150101 2호선 0238 합정 17406 15241\n",
      "20150101 3호선 0309 지축 515 538\n",
      "20150101 3호선 0310 구파발 6879 6260\n",
      "20150101 3호선 0311 연신내 20031 19470\n",
      "20150101 3호선 0312 불광 9519 11029\n",
      "20150101 4호선 0425 회현 7465 7574\n",
      "20150101 4호선 0426 서울역 3943 10823\n",
      "20150101 경부선 1002 남영 4340 4535\n",
      "20150101 경부선 1003 용산 28980 27684\n",
      "20150101 경부선 1004 노량진 23021 23862\n",
      "20150101 경부선 1005 대방 6360 6476\n",
      "20150101 경부선 1006 영등포 37247 36102\n",
      "20150101 경원선 1008 이촌 1940 1507\n",
      "20150101 경원선 1009 서빙고 911 1000\n",
      "20150101 경원선 1010 한남 1885 1863\n",
      "20150101 경원선 1011 옥수 43 37\n"
     ]
    }
   ],
   "source": [
    "for i in _subRdd.map(lambda x:x.split(',')).collect():\n",
    "    for j in i:\n",
    "        print j,\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_subRdd.map(lambda x:int(x[3])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6199,\n",
       " 7982,\n",
       " 17406,\n",
       " 515,\n",
       " 6879,\n",
       " 20031,\n",
       " 9519,\n",
       " 7465,\n",
       " 3943,\n",
       " 4340,\n",
       " 28980,\n",
       " 23021,\n",
       " 6360,\n",
       " 37247,\n",
       " 1940,\n",
       " 911,\n",
       " 1885,\n",
       " 43]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_subRdd.map(lambda x:x.split(',')).map(lambda x:int(x[4])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 총 승차인원의 합계를 reduce()로 합산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184666"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_subRdd.map(lambda x:x.split(',')).map(lambda x:(int(x[4]))).reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 호선별 승차인원 평균을 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "_subLineByPassengers=_subRdd.map(lambda x:x.split(',')).map(lambda x: (x[1],int(x[4])))\n",
    "sum_counts = _subLineByPassengers.combineByKey(\n",
    "    (lambda x: (x, 1)), # the initial value, with value x and count 1\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)), # how to combine a pair value with the accumulator: sum value, and increment count\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4호선 (11408, 2)\n",
      "2호선 (31587, 3)\n",
      "경원선 (4779, 4)\n",
      "3호선 (36944, 4)\n",
      "경부선 (99948, 5)\n"
     ]
    }
   ],
   "source": [
    "for i in sum_counts.collect():\n",
    "    for each in i:\n",
    "        print each,\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4호선 11408 2\n",
      "2호선 31587 3\n",
      "경원선 4779 4\n",
      "3호선 36944 4\n",
      "경부선 99948 5\n"
     ]
    }
   ],
   "source": [
    "for i in sum_counts.collect():\n",
    "    print i[0],i[1][0],i[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "averageByKey = sum_counts.map(lambda (key,(sum,count)):(key,float(sum)/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2\\xed\\x98\\xb8\\xec\\x84\\xa0': 10529.0,\n",
       " '3\\xed\\x98\\xb8\\xec\\x84\\xa0': 9236.0,\n",
       " '4\\xed\\x98\\xb8\\xec\\x84\\xa0': 5704.0,\n",
       " '\\xea\\xb2\\xbd\\xeb\\xb6\\x80\\xec\\x84\\xa0': 19989.6,\n",
       " '\\xea\\xb2\\xbd\\xec\\x9b\\x90\\xec\\x84\\xa0': 1194.75}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averageByKey.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4호선 5704.0\n",
      "2호선 10529.0\n",
      "경원선 1194.75\n",
      "3호선 9236.0\n",
      "경부선 19989.6\n"
     ]
    }
   ],
   "source": [
    "for i in averageByKey.collect():\n",
    "    for j in i:\n",
    "        print j,\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sub/ds_spark_rdd_hello.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile sub/ds_spark_rdd_hello.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pyspark\n",
    "def doIt():\n",
    "    print \"---------RESULT-----------\"\n",
    "    print spark.version\n",
    "    spark.conf.set(\"spark.logConf\",\"false\")\n",
    "    rdd=spark.sparkContext.parallelize(range(1000), 10)\n",
    "    print \"mean=\",rdd.mean()\n",
    "    nums = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "    squared = nums.map(lambda x: x * x).collect()\n",
    "    for num in squared:\n",
    "        print \"%i \" % (num)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF (Term Frequency)\n",
    "단어빈도를 계산하기 위해 HashingTF를 사용할 수 있다. 단어ID로 Hash 알고리즘에 따라 무작위 번호를 생성하고, 단어빈도를 생성한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어분리해서 RDD 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiRdd3 = spark.sparkContext.textFile(\"data/ds_spark_wiki.txt\")\\\n",
    "    .map(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1048576, {253068: 1.0}),\n",
       " SparseVector(1048576, {36751: 1.0, 50570: 1.0, 68380: 1.0, 415281: 1.0, 511377: 1.0, 728364: 1.0, 862087: 1.0, 938426: 1.0, 999480: 1.0}),\n",
       " SparseVector(1048576, {63234: 1.0, 340190: 1.0, 357478: 1.0, 375592: 1.0, 458138: 1.0, 486171: 1.0, 598772: 1.0}),\n",
       " SparseVector(1048576, {938426: 4.0, 999480: 4.0}),\n",
       " SparseVector(1048576, {486171: 4.0, 1016271: 4.0}),\n",
       " SparseVector(1048576, {36757: 1.0, 225801: 1.0, 323305: 1.0, 453405: 1.0, 498679: 1.0, 518030: 1.0, 688842: 1.0, 762570: 1.0, 959994: 1.0}),\n",
       " SparseVector(1048576, {420843: 1.0, 550676: 1.0, 725041: 1.0, 782544: 1.0, 938426: 1.0, 959994: 2.0, 991590: 1.0, 993084: 1.0, 996703: 1.0, 999480: 1.0}),\n",
       " SparseVector(1048576, {50573: 1.0, 263739: 1.0, 892834: 1.0, 1014710: 1.0, 1035538: 1.0}),\n",
       " SparseVector(1048576, {3932: 1.0, 36751: 1.0, 192182: 1.0, 358969: 1.0, 363244: 1.0, 496856: 1.0, 546913: 1.0, 938426: 1.0, 951974: 1.0}),\n",
       " SparseVector(1048576, {69621: 1.0, 157580: 1.0, 219357: 1.0, 297436: 1.0, 715648: 1.0})]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(wikiRdd3)\n",
    "tf.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
