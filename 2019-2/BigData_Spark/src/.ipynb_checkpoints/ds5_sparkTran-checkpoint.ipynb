{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스파크 기본설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "#home=os.path.expanduser(\"~\") # HOME이 설정되어 있지 않으면 expanduser('~')를 사용한다.\n",
    "#osn.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python\"\n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(os.path.expanduser(\"~\"),r\"C:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\")\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(\"spark.sql.warehouse.dir\", r\"C:\\Users\\user\\MyStudySpace\\2019-2\\BigData_Spark\\src\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense veotors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dv= np.array([1.0, 2.1, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,2.1,3.0] <class 'pyspark.mllib.linalg.DenseVector'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "dv1mllib=Vectors.dense([1.0, 2.1, 3])\n",
    "print dv1mllib, type(dv1mllib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,2.1,3.0]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dv1ml = Vectors.dense([1.0, 2.1, 3])\n",
    "print dv1ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**product, dot, norm**과 같은 벡터 연산을 할 수도 있다. 결과 값은 numpy와 동일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.41\n"
     ]
    }
   ],
   "source": [
    "print dv1ml.dot(dv1ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.41"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(dv,dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,4.41,9.0]\n"
     ]
    }
   ],
   "source": [
    "print dv1ml*dv1ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse vecotors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 3.]\n"
     ]
    }
   ],
   "source": [
    "sv1 = Vectors.sparse(3, [1, 2], [1.0, 3.0])\n",
    "print sv1.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 2]\n",
      " [0 0 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "\n",
    "row = np.array([0, 0, 1, 2, 2, 2])\n",
    "col = np.array([0, 2, 2, 0, 1, 2])\n",
    "data = np.array([1, 2, 3, 4, 5, 6])\n",
    "mtx = sps.csc_matrix((data, (row, col)), shape=(3, 3))\n",
    "print mtx.todense()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RowMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0, 3.0], [1.1, 2.1, 3.1], [1.2, 2.2, 3.3]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [[1.0,2.0,3.0],[1.1,2.1,3.1],[1.2,2.2,3.3]]\n",
    "my=spark.sparkContext.parallelize(p)\n",
    "\n",
    "my.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RowMatrix는 pyspark.mllib.linalg.distributed에서 제공되는 분산벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "rm=RowMatrix(my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.mllib.linalg.distributed.RowMatrix'>\n"
     ]
    }
   ],
   "source": [
    "print type(rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([1.0, 2.0, 3.0]),\n",
       " DenseVector([1.1, 2.1, 3.1]),\n",
       " DenseVector([1.2, 2.2, 3.3])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm.rows.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeled Point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분류 및 회귀분석에 사용되는 데이터 타잎\n",
    "- 형태 : [Label,[f1,f2,......,fn]]  // label, features로 구성\n",
    "    - [f1,f2,......,fn] : 학습 데이터\n",
    "    - 학습한 후에는 Label 제거\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0,[1.0,2.0,3.0])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "print LabeledPoint(1.0, [1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1992.0,(10,[0,1,2],[3.0,5.5,10.0]))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "print LabeledPoint(1992, Vectors.sparse(10, {0: 3.0, 1:5.5, 2: 10.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "서로 다른 패키지의 데이터타잎 mllib LabeledPoint와 ml Vectors를 혼용하면, 형변환 오류가 발생\n",
    "- fromML(ml벡터)사용하여 mllib 입력받는 labeledpoint에 사용가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.1,3.0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "LabeledPoint(1.0, dv1mllib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.1,3.0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "LabeledPoint(1.0, Vectors.fromML(dv1ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame에서 Labeled Point\n",
    "- Python list에서 DataFrame 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=1, _2=[1.0, 2.0, 3.0]),\n",
       " Row(_1=1, _2=[1.1, 2.1, 3.1]),\n",
       " Row(_1=0, _2=[1.2, 2.2, 3.3])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [[1,[1.0,2.0,3.0]],[1,[1.1,2.1,3.1]],[0,[1.2,2.2,3.3]]]\n",
    "trainDf=spark.createDataFrame(p)\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1.0, 2.0, 3.0]), label=1.0),\n",
       " Row(features=DenseVector([1.1, 2.1, 3.1]), label=1.0),\n",
       " Row(features=DenseVector([1.2, 2.2, 3.3]), label=0.0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "p = [LabeledPoint(1,[1.0,2.0,3.0]),\n",
    "     LabeledPoint(1,[1.1,2.1,3.1]),\n",
    "     LabeledPoint(0,[1.2,2.2,3.3])]\n",
    "trainDf=spark.createDataFrame(p)\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "schema를 사용해서 DataFrame을 생성\n",
    "- 'label'은 DoubleType\n",
    "- 'features'는 VectorType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector, VectorUDT\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "_rdd = spark.sparkContext.parallelize([\n",
    "    (0.0, SparseVector(4, {1: 1.0, 3: 5.5})),\n",
    "    (1.0, SparseVector(4, {0: -1.0, 2: 0.5}))])\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"label\", DoubleType(), True),\n",
    "    StructField(\"features\", VectorUDT(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf=_rdd.toDF(schema)\n",
    "trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sparse에서 dense vector로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "#from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "from pyspark.mllib.linalg import DenseVector, VectorUDT\n",
    "#myudf=udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "#myudf=udf(lambda x: Vectors.dense(x))\n",
    "myudf=udf(lambda x: DenseVector(x.toArray()), VectorUDT())\n",
    "_trainDf2=trainDf.withColumn('dvf',myudf(trainDf.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- dvf: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_trainDf2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+\n",
      "|label|            features|               dvf|\n",
      "+-----+--------------------+------------------+\n",
      "|  0.0| (4,[1,3],[1.0,5.5])| [0.0,1.0,0.0,5.5]|\n",
      "|  1.0|(4,[0,2],[-1.0,0.5])|[-1.0,0.0,0.5,0.0]|\n",
      "+-----+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_trainDf2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## maxtrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Matrix, Matrices\n",
    "\n",
    "# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\n",
    "dm = Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Create a sparse matrix ((9.0, 0.0), (0.0, 8.0), (0.0, 6.0))\n",
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## libsvm format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsvm=os.path.join(os.environ[\"SPARK_HOME\"],'data','mllib','sample_libsvm_data.txt')\n",
    "dfsvm = spark.read.format(\"libsvm\").load(fsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dfsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
