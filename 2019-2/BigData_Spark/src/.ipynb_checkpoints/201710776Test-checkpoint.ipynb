{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 201710776 박희지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 스파크 기본설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "#home=os.path.expanduser(\"~\") # HOME이 설정되어 있지 않으면 expanduser('~')를 사용한다.\n",
    "#osn.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python\"\n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(os.path.expanduser(\"~\"),r\"C:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\")\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(\"spark.sql.warehouse.dir\", r\"C:\\Users\\user\\MyStudySpace\\2019-2\\BigData_Spark\\src\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 한 글자 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "존경하는\n",
      "국민\n",
      "여러분,\n",
      "경찰관\n",
      "여러분,\n",
      "일흔네\n",
      "‘경찰의\n",
      "날’입니다.\n",
      "국민의\n",
      "안전을\n",
      "위해\n",
      "밤낮없이\n",
      "애쓰시는\n",
      "전국의\n",
      "15만\n",
      "경찰관\n",
      "여러분께\n",
      "먼저\n",
      "감사를\n",
      "드립니다.\n",
      "전몰·순직\n",
      "경찰관들의\n",
      "고귀한\n",
      "희생에\n",
      "경의를\n",
      "표합니다.\n",
      "유가족\n",
      "여러분께\n",
      "위로의\n",
      "마음을\n",
      "전합니다.\n",
      "오늘\n",
      "홍조근정훈장을\n",
      "받으신\n",
      "중앙경찰학교장\n",
      "이은정\n",
      "치안감님,\n",
      "근정포장을\n",
      "받으신\n",
      "광주남부경찰서\n",
      "김동현\n",
      "경감님을\n",
      "비롯한\n",
      "수상자\n",
      "여러분께\n",
      "각별한\n",
      "축하와\n",
      "감사를\n",
      "드립니다.\n",
      "또한\n",
      "경찰\n",
      "영웅으로\n",
      "추서되신\n",
      "차일혁,\n",
      "최중락님께\n",
      "국민의\n",
      "사랑을\n",
      "전해드립니다.\n",
      "사랑하는\n",
      "경찰관\n",
      "여러분,\n",
      "여러분의\n",
      "헌신적\n",
      "노력으로\n",
      "우리의\n",
      "치안은\n",
      "좋아졌습니다.\n",
      "지난해\n",
      "범죄\n",
      "발생은\n",
      "2015년에\n",
      "비해\n",
      "15.1%\n",
      "줄었습니다.\n",
      "같은\n",
      "기간\n",
      "교통사고\n",
      "사망자는\n",
      "18.2%\n",
      "감소했습니다.\n",
      "치안의\n",
      "개선은\n",
      "국민의\n",
      "체감으로\n",
      "나타나고\n",
      "있습니다.\n",
      "올해\n",
      "상반기\n",
      "국민의\n",
      "체감안전도는\n",
      "74.5점으로\n",
      "역대\n",
      "최고를\n",
      "기록했습니다.\n",
      "범죄안전도는\n",
      "처음으로\n",
      "80점을\n",
      "넘었습니다.\n",
      "한국을\n",
      "찾는\n",
      "외국\n",
      "관광객들도\n",
      "우리의\n",
      "치안을\n",
      "가장\n",
      "좋게\n",
      "평가합니다.\n",
      "한국의\n",
      "무엇이\n",
      "좋았느냐는\n",
      "물음에\n",
      "외국\n",
      "관광객들은\n",
      "7년\n",
      "연속으로\n",
      "치안이\n",
      "가장\n",
      "좋았다고\n",
      "응답했습니다.\n",
      "개발도상국들은\n",
      "우리의\n",
      "경찰을\n",
      "모범으로\n",
      "삼으려\n",
      "합니다.\n",
      "올해는\n",
      "‘경찰의\n",
      "날’에\n",
      "맞춰\n",
      "국제치안산업박람회와\n",
      "서울국제경찰청장회의가\n",
      "함께\n",
      "열립니다.\n",
      "우리의\n",
      "치안\n",
      "발전과\n",
      "치안산업\n",
      "발전이\n",
      "세계에\n",
      "널리\n",
      "알려지게\n",
      "것입니다.\n",
      "자랑스러운\n",
      "경찰관\n",
      "여러분,\n",
      "경찰헌장은\n",
      "“나라와\n",
      "겨레를\n",
      "위하여\n",
      "충성”을\n",
      "다한다는\n",
      "다짐으로\n",
      "시작합니다.\n",
      "헌장처럼\n",
      "우리\n",
      "경찰은\n",
      "‘나라와\n",
      "겨레를\n",
      "위한\n",
      "충성’의\n",
      "길을\n",
      "걸으려\n",
      "노력해\n",
      "왔습니다.\n",
      "대한민국\n",
      "경찰은\n",
      "1945년\n",
      "광복\n",
      "직후에\n",
      "공식\n",
      "탄생했습니다.\n",
      "그러나\n",
      "뿌리는\n",
      "대한민국\n",
      "임시정부에\n",
      "닿아\n",
      "있습니다.\n",
      "임시정부\n",
      "초대\n",
      "경무국장\n",
      "백범\n",
      "김구\n",
      "선생과\n",
      "나석주,\n",
      "나창헌,\n",
      "유상근\n",
      "의사\n",
      "임시정부\n",
      "경찰은\n",
      "앞장서서\n",
      "일제와\n",
      "싸웠습니다.\n",
      "일본\n",
      "관헌에게\n",
      "폭탄을\n",
      "던지고,\n",
      "밀정을\n",
      "응징하며,\n",
      "임정\n",
      "요인들을\n",
      "보호했습니다.\n",
      "광복\n",
      "이후\n",
      "6‧25전쟁에서도\n",
      "경찰은\n",
      "국군과\n",
      "함께\n",
      "피를\n",
      "흘렸습니다.\n",
      "전쟁에서\n",
      "1만여\n",
      "명의\n",
      "경찰관이\n",
      "목숨을\n",
      "잃었습니다.\n",
      "후로도\n",
      "경찰은\n",
      "국민의\n",
      "안전을\n",
      "지키고\n",
      "국가의\n",
      "안보를\n",
      "도왔습니다.\n",
      "역대\n",
      "경찰의\n",
      "헌신에\n",
      "대해\n",
      "국민과\n",
      "함께\n",
      "거듭\n",
      "감사의\n",
      "말씀을\n",
      "드립니다.\n",
      "감사합니다.\n",
      "그러나\n",
      "잘못도\n",
      "없지는\n",
      "않았습니다.\n",
      "한때\n",
      "경찰은\n",
      "공권력을\n",
      "무리하게\n",
      "집행하며\n",
      "국민의\n",
      "인권을\n",
      "훼손했습니다.\n",
      "부실하거나\n",
      "불공정한\n",
      "수사로\n",
      "국민의\n",
      "지탄을\n",
      "받은\n",
      "적도\n",
      "있습니다.\n",
      "무기력한\n",
      "집행으로\n",
      "국민께\n",
      "걱정을\n",
      "드리기도\n",
      "했습니다.\n",
      "지금\n",
      "경찰은\n",
      "과거를\n",
      "돌아보며\n",
      "국민과\n",
      "국가에\n",
      "충성하는\n",
      "경찰로\n",
      "거듭나려고\n",
      "노력하고\n",
      "있습니다.\n",
      "경찰은\n",
      "문재인\n",
      "정부\n",
      "들어\n",
      "가장\n",
      "먼저\n",
      "개혁위원회를\n",
      "만들고\n",
      "자체개혁에\n",
      "나섰습니다.\n",
      "경찰의\n",
      "개혁을\n",
      "국민은\n",
      "기대로\n",
      "주목하고\n",
      "있습니다.\n",
      "검경\n",
      "수사권\n",
      "조정과\n",
      "자치경찰제\n",
      "도입이\n",
      "국회에서\n",
      "논의되고\n",
      "있습니다.\n",
      "국회가\n",
      "조속히\n",
      "입법을\n",
      "매듭지어\n",
      "주시기\n",
      "바랍니다.\n",
      "그리하여\n",
      "경찰이\n",
      "중립성,\n",
      "공정성,\n",
      "전문성을\n",
      "갖추고\n",
      "본연의\n",
      "임무를\n",
      "충실히\n",
      "수행하는\n",
      "선진경찰로\n",
      "더욱\n",
      "발전해\n",
      "주기를\n",
      "소망합니다.\n",
      "정부는\n",
      "경찰의\n",
      "근무여건을\n",
      "개선하기\n",
      "위해\n",
      "노력하고\n",
      "있습니다.\n",
      "이미\n",
      "경찰관\n",
      "8,572명을\n",
      "늘렸고,\n",
      "앞으로도\n",
      "충원을\n",
      "계속할\n",
      "것입니다.\n",
      "특히\n",
      "일선\n",
      "경찰의\n",
      "근무환경을\n",
      "개선하겠습니다.\n",
      "정부는\n",
      "누구도\n",
      "위에\n",
      "군림하지\n",
      "못하는\n",
      "법치주의를\n",
      "확립하고자\n",
      "합니다.\n",
      "그러자면\n",
      "검찰과\n",
      "경찰이\n",
      "법을\n",
      "누구에게나\n",
      "엄정하고\n",
      "공정하게\n",
      "집행해야\n",
      "됩니다.\n",
      "수사\n",
      "또한\n",
      "엄정하고\n",
      "공정해야\n",
      "합니다.\n",
      "동시에\n",
      "검찰과\n",
      "경찰\n",
      "스스로도\n",
      "법을\n",
      "엄격히\n",
      "준수해야\n",
      "합니다.\n",
      "특히\n",
      "공권력이\n",
      "인권의\n",
      "제약을\n",
      "수반하는\n",
      "경우에는\n",
      "절제하며\n",
      "행사하는\n",
      "것이\n",
      "마땅합니다.\n",
      "검찰개혁과\n",
      "경찰개혁은\n",
      "미룰\n",
      "없는\n",
      "시대적\n",
      "과제가\n",
      "됐습니다.\n",
      "경찰헌장은\n",
      "따뜻한\n",
      "경찰,\n",
      "의로운\n",
      "경찰,\n",
      "공정한\n",
      "경찰을\n",
      "다짐합니다.\n",
      "흔들림\n",
      "없이\n",
      "길로\n",
      "가시기\n",
      "바랍니다.\n",
      "국민이\n",
      "여러분을\n",
      "응원할\n",
      "것입니다.\n",
      "행사를\n",
      "준비하신\n",
      "민갑룡\n",
      "경찰청장님과\n",
      "관계자\n",
      "여러분,\n",
      "고맙습니다.\n",
      "함께하신\n",
      "진영\n",
      "행정안전부\n",
      "장관님,\n",
      "이용범\n",
      "인천시의회\n",
      "의장님,\n",
      "박정훈\n",
      "경찰위원장님,\n",
      "강영규\n",
      "경우회장님과\n",
      "역대\n",
      "경찰청장님,\n",
      "그리고\n",
      "우리\n",
      "시민\n",
      "경찰님들을\n",
      "비롯한\n",
      "내빈\n",
      "여러분,\n",
      "고맙습니다.\n",
      "감사합니다.\n"
     ]
    }
   ],
   "source": [
    "poliRdd = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\", \"20191021_policeAddress.txt\"))\n",
    "rm1_poliRdd = poliRdd.flatMap(lambda x:x.split())\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .filter(lambda x: len(x) > 1)\n",
    "\n",
    "for word in rm1_poliRdd.collect():\n",
    "    print word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 빈도순 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 경찰은\n",
      "7 있습니다.\n",
      "7 국민의\n",
      "6 여러분,\n",
      "5 경찰관\n",
      "4 합니다.\n",
      "4 우리의\n",
      "4 경찰의\n",
      "3 것입니다.\n",
      "3 함께\n",
      "3 여러분께\n",
      "3 역대\n",
      "3 가장\n",
      "3 드립니다.\n",
      "2 외국\n",
      "2 경찰,\n",
      "2 고맙습니다.\n",
      "2 받으신\n",
      "2 위해\n",
      "2 감사를\n",
      "2 우리\n",
      "2 비롯한\n",
      "2 특히\n",
      "2 경찰\n",
      "2 노력하고\n",
      "2 또한\n",
      "2 대한민국\n",
      "2 ‘경찰의\n",
      "2 임시정부\n",
      "2 경찰헌장은\n",
      "2 그러나\n",
      "2 정부는\n",
      "2 검찰과\n",
      "2 엄정하고\n",
      "2 국민과\n",
      "2 겨레를\n",
      "2 바랍니다.\n",
      "2 광복\n",
      "2 법을\n",
      "2 경찰을\n",
      "2 경찰이\n",
      "2 감사합니다.\n",
      "2 먼저\n",
      "2 안전을\n",
      "1 1945년\n",
      "1 18.2%\n",
      "1 평가합니다.\n",
      "1 사망자는\n",
      "1 치안\n",
      "1 경찰로\n",
      "1 국회에서\n",
      "1 한때\n",
      "1 조속히\n",
      "1 공정성,\n",
      "1 감소했습니다.\n",
      "1 수반하는\n",
      "1 1만여\n",
      "1 정부\n",
      "1 발전이\n",
      "1 선생과\n",
      "1 좋았다고\n",
      "1 치안산업\n",
      "1 근정포장을\n",
      "1 의사\n",
      "1 다짐합니다.\n",
      "1 80점을\n",
      "1 자치경찰제\n",
      "1 거듭나려고\n",
      "1 수사\n",
      "1 왔습니다.\n",
      "1 주목하고\n",
      "1 “나라와\n",
      "1 경찰관이\n",
      "1 체감으로\n",
      "1 개선은\n",
      "1 검경\n",
      "1 이은정\n",
      "1 없는\n",
      "1 일흔네\n",
      "1 서울국제경찰청장회의가\n",
      "1 위한\n",
      "1 그리하여\n",
      "1 도왔습니다.\n",
      "1 집행으로\n",
      "1 다짐으로\n",
      "1 경찰청장님과\n",
      "1 경우에는\n",
      "1 행정안전부\n",
      "1 개발도상국들은\n",
      "1 의장님,\n",
      "1 과제가\n",
      "1 것이\n",
      "1 뿌리는\n",
      "1 영웅으로\n",
      "1 잘못도\n",
      "1 계속할\n",
      "1 무기력한\n",
      "1 좋았느냐는\n",
      "1 싸웠습니다.\n",
      "1 국민\n",
      "1 지키고\n",
      "1 경감님을\n",
      "1 군림하지\n",
      "1 본연의\n",
      "1 근무환경을\n",
      "1 전국의\n",
      "1 물음에\n",
      "1 목숨을\n",
      "1 삼으려\n",
      "1 수상자\n",
      "1 같은\n",
      "1 교통사고\n",
      "1 더욱\n",
      "1 탄생했습니다.\n",
      "1 ‘나라와\n",
      "1 국회가\n",
      "1 엄격히\n",
      "1 잃었습니다.\n",
      "1 근무여건을\n",
      "1 시민\n",
      "1 문재인\n",
      "1 개혁을\n",
      "1 발전해\n",
      "1 누구에게나\n",
      "1 헌신적\n",
      "1 기간\n",
      "1 세계에\n",
      "1 홍조근정훈장을\n",
      "1 7년\n",
      "1 기록했습니다.\n",
      "1 이미\n",
      "1 여러분의\n",
      "1 경찰님들을\n",
      "1 여러분을\n",
      "1 경찰개혁은\n",
      "1 줄었습니다.\n",
      "1 무리하게\n",
      "1 걱정을\n",
      "1 누구도\n",
      "1 도입이\n",
      "1 올해\n",
      "1 박정훈\n",
      "1 국제치안산업박람회와\n",
      "1 발전과\n",
      "1 나창헌,\n",
      "1 집행하며\n",
      "1 모범으로\n",
      "1 그리고\n",
      "1 위에\n",
      "1 공식\n",
      "1 소망합니다.\n",
      "1 지금\n",
      "1 의로운\n",
      "1 관광객들은\n",
      "1 검찰개혁과\n",
      "1 임무를\n",
      "1 자랑스러운\n",
      "1 흘렸습니다.\n",
      "1 만들고\n",
      "1 8,572명을\n",
      "1 불공정한\n",
      "1 법치주의를\n",
      "1 인권을\n",
      "1 이용범\n",
      "1 충성’의\n",
      "1 일본\n",
      "1 선진경찰로\n",
      "1 따뜻한\n",
      "1 최고를\n",
      "1 개혁위원회를\n",
      "1 늘렸고,\n",
      "1 수행하는\n",
      "1 백범\n",
      "1 앞으로도\n",
      "1 경찰관들의\n",
      "1 치안감님,\n",
      "1 없이\n",
      "1 행사를\n",
      "1 광주남부경찰서\n",
      "1 관계자\n",
      "1 추서되신\n",
      "1 길을\n",
      "1 수사로\n",
      "1 오늘\n",
      "1 흔들림\n",
      "1 날’에\n",
      "1 전몰·순직\n",
      "1 제약을\n",
      "1 충실히\n",
      "1 국가에\n",
      "1 전문성을\n",
      "1 전합니다.\n",
      "1 들어\n",
      "1 없지는\n",
      "1 못하는\n",
      "1 알려지게\n",
      "1 6‧25전쟁에서도\n",
      "1 최중락님께\n",
      "1 밀정을\n",
      "1 유가족\n",
      "1 장관님,\n",
      "1 국민께\n",
      "1 좋게\n",
      "1 했습니다.\n",
      "1 대해\n",
      "1 범죄\n",
      "1 앞장서서\n",
      "1 나섰습니다.\n",
      "1 충성”을\n",
      "1 각별한\n",
      "1 나석주,\n",
      "1 그러자면\n",
      "1 직후에\n",
      "1 비해\n",
      "1 시대적\n",
      "1 공정해야\n",
      "1 넘었습니다.\n",
      "1 15.1%\n",
      "1 공권력을\n",
      "1 공정하게\n",
      "1 경찰위원장님,\n",
      "1 공권력이\n",
      "1 기대로\n",
      "1 길로\n",
      "1 던지고,\n",
      "1 적도\n",
      "1 다한다는\n",
      "1 민갑룡\n",
      "1 위하여\n",
      "1 명의\n",
      "1 미룰\n",
      "1 상반기\n",
      "1 처음으로\n",
      "1 입법을\n",
      "1 날’입니다.\n",
      "1 노력해\n",
      "1 동시에\n",
      "1 경찰청장님,\n",
      "1 연속으로\n",
      "1 갖추고\n",
      "1 밤낮없이\n",
      "1 개선하기\n",
      "1 애쓰시는\n",
      "1 인천시의회\n",
      "1 절제하며\n",
      "1 김동현\n",
      "1 관헌에게\n",
      "1 임정\n",
      "1 가시기\n",
      "1 찾는\n",
      "1 노력으로\n",
      "1 개선하겠습니다.\n",
      "1 거듭\n",
      "1 국민이\n",
      "1 돌아보며\n",
      "1 피를\n",
      "1 받은\n",
      "1 걸으려\n",
      "1 지난해\n",
      "1 헌장처럼\n",
      "1 행사하는\n",
      "1 국민은\n",
      "1 인권의\n",
      "1 됐습니다.\n",
      "1 자체개혁에\n",
      "1 드리기도\n",
      "1 않았습니다.\n",
      "1 말씀을\n",
      "1 부실하거나\n",
      "1 초대\n",
      "1 수사권\n",
      "1 사랑하는\n",
      "1 전쟁에서\n",
      "1 공정한\n",
      "1 전해드립니다.\n",
      "1 올해는\n",
      "1 경우회장님과\n",
      "1 진영\n",
      "1 사랑을\n",
      "1 일선\n",
      "1 마음을\n",
      "1 나타나고\n",
      "1 축하와\n",
      "1 일제와\n",
      "1 15만\n",
      "1 응원할\n",
      "1 경무국장\n",
      "1 국가의\n",
      "1 됩니다.\n",
      "1 널리\n",
      "1 강영규\n",
      "1 요인들을\n",
      "1 매듭지어\n",
      "1 충성하는\n",
      "1 지탄을\n",
      "1 준수해야\n",
      "1 폭탄을\n",
      "1 희생에\n",
      "1 국군과\n",
      "1 유상근\n",
      "1 표합니다.\n",
      "1 닿아\n",
      "1 한국을\n",
      "1 한국의\n",
      "1 과거를\n",
      "1 시작합니다.\n",
      "1 후로도\n",
      "1 중립성,\n",
      "1 확립하고자\n",
      "1 2015년에\n",
      "1 함께하신\n",
      "1 범죄안전도는\n",
      "1 논의되고\n",
      "1 주기를\n",
      "1 응징하며,\n",
      "1 충원을\n",
      "1 주시기\n",
      "1 스스로도\n",
      "1 차일혁,\n",
      "1 준비하신\n",
      "1 헌신에\n",
      "1 존경하는\n",
      "1 훼손했습니다.\n",
      "1 중앙경찰학교장\n",
      "1 집행해야\n",
      "1 좋아졌습니다.\n",
      "1 고귀한\n",
      "1 치안은\n",
      "1 열립니다.\n",
      "1 치안을\n",
      "1 감사의\n",
      "1 내빈\n",
      "1 보호했습니다.\n",
      "1 발생은\n",
      "1 이후\n",
      "1 치안의\n",
      "1 체감안전도는\n",
      "1 무엇이\n",
      "1 맞춰\n",
      "1 위로의\n",
      "1 김구\n",
      "1 경의를\n",
      "1 치안이\n",
      "1 74.5점으로\n",
      "1 응답했습니다.\n",
      "1 관광객들도\n",
      "1 임시정부에\n",
      "1 조정과\n",
      "1 안보를\n",
      "1 마땅합니다.\n"
     ]
    }
   ],
   "source": [
    "sortedList = rm1_poliRdd\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .map(lambda x:(x[1],x[0]))\\\n",
    "    .sortByKey(False)\\\n",
    "    .collect()\n",
    "\n",
    "for i in sortedList:\n",
    "    print i[0],i[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 72.0 failed 1 times, most recent failure: Lost task 0.0 in stage 72.0 (TID 1067, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-78-d987ba773621>\", line 1, in <lambda>\nIndexError: list index out of range\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\r\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:893)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1911)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:892)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-78-d987ba773621>\", line 1, in <lambda>\nIndexError: list index out of range\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\r\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:893)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-d987ba773621>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbusRdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparkContext\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"BUS_STATION_BOARDING_MONTH_201909_small.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbusRdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    774\u001b[0m         \"\"\"\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 776\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    777\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 933\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    311\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    313\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 72.0 failed 1 times, most recent failure: Lost task 0.0 in stage 72.0 (TID 1067, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-78-d987ba773621>\", line 1, in <lambda>\nIndexError: list index out of range\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\r\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:893)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1911)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:892)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"C:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"C:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-78-d987ba773621>\", line 1, in <lambda>\nIndexError: list index out of range\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\r\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\r\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\r\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:893)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\r\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "busRdd = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\", \"BUS_STATION_BOARDING_MONTH_201909_small.csv\"))\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:x.split(','))\\\n",
    "    .map(lambda x: (x[1], x[8], x[9]))\n",
    "\n",
    "\n",
    "busRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "busRdd = spark.sparkContext\\\n",
    "    .reduceByKey(lambda (x[0],x[1],x[2]):x+y)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 5행 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|                date| count|\n",
      "+--------------------+------+\n",
      "|2018-01-01 00:00:...|  4950|\n",
      "|2018-01-02 00:00:...|  7136|\n",
      "|2018-01-03 00:00:...|  7156|\n",
      "|2018-01-04 00:00:...|  7102|\n",
      "|2018-01-05 00:00:...|  7705|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/seoulBicycleDailyCount_2018_201903.csv')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----+-----+---+\n",
      "|                Date|Count|year|month|day|\n",
      "+--------------------+-----+----+-----+---+\n",
      "|2018-01-01 00:00:...| 4950|2018|   01| 01|\n",
      "|2018-01-02 00:00:...| 7136|2018|   01| 02|\n",
      "|2018-01-03 00:00:...| 7156|2018|   01| 03|\n",
      "|2018-01-04 00:00:...| 7102|2018|   01| 04|\n",
      "|2018-01-05 00:00:...| 7705|2018|   01| 05|\n",
      "|2018-01-06 00:00:...| 5681|2018|   01| 06|\n",
      "|2018-01-07 00:00:...| 5220|2018|   01| 07|\n",
      "|2018-01-08 00:00:...| 6309|2018|   01| 08|\n",
      "|2018-01-09 00:00:...| 5988|2018|   01| 09|\n",
      "|2018-01-10 00:00:...| 4476|2018|   01| 10|\n",
      "|2018-01-11 00:00:...| 4337|2018|   01| 11|\n",
      "|2018-01-12 00:00:...| 4401|2018|   01| 12|\n",
      "|2018-01-13 00:00:...| 3756|2018|   01| 13|\n",
      "|2018-01-14 00:00:...| 4675|2018|   01| 14|\n",
      "|2018-01-15 00:00:...| 6993|2018|   01| 15|\n",
      "|2018-01-16 00:00:...| 7421|2018|   01| 16|\n",
      "|2018-01-17 00:00:...| 6990|2018|   01| 17|\n",
      "|2018-01-18 00:00:...| 7054|2018|   01| 18|\n",
      "|2018-01-19 00:00:...| 8329|2018|   01| 19|\n",
      "|2018-01-20 00:00:...| 6148|2018|   01| 20|\n",
      "+--------------------+-----+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "df=df\\\n",
    "    .withColumn(\"year\",df.Date.substr(1, 4))\\\n",
    "    .withColumn(\"month\",df.Date.substr(6, 2))\\\n",
    "    .withColumn(\"day\",df.Date.substr(9, 2))\n",
    "# df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 년별 분기별 대여건수 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+------+------+------+-------+-------+-------+-------+-------+------+------+\n",
      "|year|    01|    02|    03|    04|    05|     06|     07|     08|     09|     10|    11|    12|\n",
      "+----+------+------+------+------+------+-------+-------+-------+-------+-------+------+------+\n",
      "|2019|495573|471543|904819|  null|  null|   null|   null|   null|   null|   null|  null|  null|\n",
      "|2018|164367|168741|462661|687885|965609|1207123|1100015|1037505|1447993|1420621|961532|500822|\n",
      "+----+------+------+------+------+------+-------+-------+-------+-------+-------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('year')\\\n",
    "    .pivot('month')\\\n",
    "    .agg({\"Count\":\"sum\"}).show()\n",
    "\n",
    "# df.groupBy('year')\\\n",
    "#     .agg({\"Count\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument 2 to map() must support iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-12fd5f307201>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mword\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mday\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'grey'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: argument 2 to map() must support iteration"
     ]
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "count = map(lambda x: x[0], df.day)\n",
    "word = map(lambda x: x[1], df.day)\n",
    "plt.barh(range(len(Count)), count, color = 'grey')\n",
    "plt.yticks(range(len(Count)), word)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
