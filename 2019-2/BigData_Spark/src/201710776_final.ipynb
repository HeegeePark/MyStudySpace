{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 201710776 박희지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "#home=os.path.expanduser(\"~\") # HOME이 설정되어 있지 않으면 expanduser('~')를 사용한다.\n",
    "#osn.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python\"\n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(os.path.expanduser(\"~\"),r\"C:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\")\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(\"spark.sql.warehouse.dir\", r\"C:\\Users\\user\\MyStudySpace\\2019-2\\BigData_Spark\\src\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitDf = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .load(os.path.join(\"data\",\"testdata.manual.2009.06.14.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------------+-------+------+--------------------+\n",
      "|_c0|_c1|                 _c2|    _c3|   _c4|                 _c5|\n",
      "+---+---+--------------------+-------+------+--------------------+\n",
      "|  4|  3|Mon May 11 03:17:...|kindle2|tpryan|@stellargirl I lo...|\n",
      "|  4|  4|Mon May 11 03:18:...|kindle2|vcu451|Reading my kindle...|\n",
      "|  4|  5|Mon May 11 03:18:...|kindle2|chadfu|Ok, first assesme...|\n",
      "+---+---+--------------------+-------+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitDf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 정서별 데이터 갯수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|_c0|count|\n",
      "+---+-----+\n",
      "|  0|  177|\n",
      "|  4|  182|\n",
      "|  2|  139|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "twitDf.groupBy(twitDf._c0).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)트윗 정서 레이블 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|_c0|\n",
      "+---+\n",
      "|  4|\n",
      "|  4|\n",
      "|  4|\n",
      "|  4|\n",
      "|  4|\n",
      "|  4|\n",
      "|  0|\n",
      "|  4|\n",
      "|  4|\n",
      "|  4|\n",
      "|  2|\n",
      "|  0|\n",
      "|  4|\n",
      "|  4|\n",
      "|  0|\n",
      "|  4|\n",
      "|  0|\n",
      "|  4|\n",
      "|  0|\n",
      "|  4|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label=twitDf.select('_c0')\n",
    "label.show()\n",
    "label.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitDf.where(twitDf['_c0'].isNull()).count()\n",
    "twitDf.where(twitDf['_c1'].isNull()).count()\n",
    "twitDf.where(twitDf['_c2'].isNull()).count()\n",
    "twitDf.where(twitDf['_c3'].isNull()).count()\n",
    "twitDf.where(twitDf['_c4'].isNull()).count()\n",
    "twitDf.where(twitDf['_c5'].isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) 중립 정서 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------------+-------+--------------+--------------------+\n",
      "|_c0|_c1|                 _c2|    _c3|           _c4|                 _c5|\n",
      "+---+---+--------------------+-------+--------------+--------------------+\n",
      "|  4|  3|Mon May 11 03:17:...|kindle2|        tpryan|@stellargirl I lo...|\n",
      "|  4|  4|Mon May 11 03:18:...|kindle2|        vcu451|Reading my kindle...|\n",
      "|  4|  5|Mon May 11 03:18:...|kindle2|        chadfu|Ok, first assesme...|\n",
      "|  4|  6|Mon May 11 03:19:...|kindle2|         SIX15|@kenburbary You'l...|\n",
      "|  4|  7|Mon May 11 03:21:...|kindle2|      yamarama|@mikefish  Fair e...|\n",
      "|  4|  8|Mon May 11 03:22:...|kindle2|  GeorgeVHulme|@richardebaker no...|\n",
      "|  0|  9|Mon May 11 03:22:...|    aig|       Seth937|Fuck this economy...|\n",
      "|  4| 10|Mon May 11 03:26:...| jquery|     dcostalis|Jquery is my new ...|\n",
      "|  4| 11|Mon May 11 03:27:...|twitter|       PJ_King|       Loves twitter|\n",
      "|  4| 12|Mon May 11 03:29:...|  obama|   mandanicole|how can you not l...|\n",
      "|  0| 14|Mon May 11 03:32:...|  obama|   kylesellers|@Karoli I firmly ...|\n",
      "|  4| 15|Mon May 11 03:33:...|  obama|   theviewfans|House Corresponde...|\n",
      "|  4| 16|Mon May 11 05:05:...|   nike|        MumsFP|Watchin Espn..Jus...|\n",
      "|  0| 17|Mon May 11 05:06:...|   nike|   vincentx24x|dear nike, stop w...|\n",
      "|  4| 18|Mon May 11 05:20:...| lebron|  cameronwylie|#lebron best athl...|\n",
      "|  0| 19|Mon May 11 05:20:...| lebron|       luv8242|I was talking to ...|\n",
      "|  4| 20|Mon May 11 05:21:...| lebron|    mtgillikin|i love lebron. ht...|\n",
      "|  0| 21|Mon May 11 05:21:...| lebron|ursecretdezire|@ludajuice Lebron...|\n",
      "|  4| 22|Mon May 11 05:21:...| lebron|     Native_01|@Pmillzz lebron I...|\n",
      "|  4| 23|Mon May 11 05:22:...| lebron|  princezzcutz|@sketchbug Lebron...|\n",
      "+---+---+--------------------+-------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-32af21099dca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtwitDf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtwitDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtwitDf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_c0'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtwitDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "twitDf=twitDf.filter(twitDf['_c0'] != 2).show()\n",
    "twitDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) 토큰화하여 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '_jdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-34dffaa1793c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0m_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"_c5\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"content_tok\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtokDf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtwitDf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtokDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'content_tok'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_jdf'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "_tokenizer = Tokenizer(inputCol=\"_c5\", outputCol=\"content_tok\")\n",
    "tokDf = _tokenizer.transform(twitDf)\n",
    "tokDf.select('content_tok').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stop = StopWordsRemover(inputCol=\"content_tok\", outputCol=\"nostops\")\n",
    "\n",
    "stopwords=list()\n",
    "_stopwords=stop.getStopWords()\n",
    "for e in _stopwords:\n",
    "    stopwords.append(e)\n",
    "\n",
    "_mystopwords=[\":)\", \"@stellargirl\", \"@kenburbary\"]\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)\n",
    "    stopwords.append(e+\".\")\n",
    "stop.setStopWords(stopwords)\n",
    "\n",
    "stopDf = stop.transform(tokDf)\n",
    "# stopDf.printSchema()\n",
    "# stopDf.select(\"nostops\").show(1,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             nostops|\n",
      "+--------------------+\n",
      "|[loooooooovvvvvve...|\n",
      "|[reading, kindle2...|\n",
      "|[ok,, first, asse...|\n",
      "|[you'll, love, ki...|\n",
      "|[@mikefish, , fai...|\n",
      "|[@richardebaker, ...|\n",
      "|[fuck, economy., ...|\n",
      "|[jquery, new, bes...|\n",
      "|    [loves, twitter]|\n",
      "|[love, obama?, ma...|\n",
      "|[check, video, --...|\n",
      "|[@karoli, firmly,...|\n",
      "|[house, correspon...|\n",
      "|[watchin, espn..j...|\n",
      "|[dear, nike,, sto...|\n",
      "|[#lebron, best, a...|\n",
      "|[talking, guy, la...|\n",
      "|[love, lebron., h...|\n",
      "|[@ludajuice, lebr...|\n",
      "|[@pmillzz, lebron...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf.select('nostops').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) 파이프라인으로 tf-idf 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "u'requirement failed: Column nostops must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually ArrayType(StringType,true).'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-78b5e06564e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mva\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopDf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mpipDf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopDf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\ml\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    111\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# must be an Estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m     62\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    208\u001b[0m         \"\"\"\n\u001b[0;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 933\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: u'requirement failed: Column nostops must be of type org.apache.spark.ml.linalg.VectorUDT@3bfc3ba7 but was actually ArrayType(StringType,true).'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "idf = IDF(inputCol=\"nostops\", outputCol=\"idf\")\n",
    "va = VectorAssembler(inputCols=[\"idf\"],\\\n",
    "                     outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[idf,va])\n",
    "\n",
    "model = pipeline.fit(stopDf)\n",
    "pipDf = model.transform(stopDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3  키 몸무게 데이터 회귀분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'\"Index\"', u' Height(Inches)\"', u' \"Weight(Pounds)\"'],\n",
       " [u'1', u' 65.78', u' 112.99'],\n",
       " [u'2', u' 71.52', u' 136.49'],\n",
       " [u'3', u' 69.40', u' 153.03'],\n",
       " [u'4', u' 68.22', u' 142.34'],\n",
       " [u'5', u' 67.79', u' 144.30'],\n",
       " [u'6', u' 68.70', u' 123.30'],\n",
       " [u'7', u' 69.80', u' 141.49'],\n",
       " [u'8', u' 70.01', u' 136.46'],\n",
       " [u'9', u' 67.90', u' 112.37'],\n",
       " [u'10', u' 66.78', u' 120.67'],\n",
       " [u'11', u' 66.49', u' 127.45'],\n",
       " [u'12', u' 67.62', u' 114.14'],\n",
       " [u'13', u' 68.30', u' 125.61'],\n",
       " [u'14', u' 67.12', u' 122.46'],\n",
       " [u'15', u' 68.28', u' 116.09'],\n",
       " [u'16', u' 71.09', u' 140.00'],\n",
       " [u'17', u' 66.46', u' 129.50'],\n",
       " [u'18', u' 68.65', u' 142.97'],\n",
       " [u'19', u' 71.23', u' 137.90'],\n",
       " [u'20', u' 67.13', u' 124.04'],\n",
       " [u'21', u' 67.83', u' 141.28'],\n",
       " [u'22', u' 68.88', u' 143.54'],\n",
       " [u'23', u' 63.48', u' 97.90'],\n",
       " [u'24', u' 68.42', u' 129.50'],\n",
       " [u'25', u' 67.63', u' 141.85'],\n",
       " [u'26', u' 67.21', u' 129.72'],\n",
       " [u'27', u' 70.84', u' 142.42'],\n",
       " [u'28', u' 67.49', u' 131.55'],\n",
       " [u'29', u' 66.53', u' 108.33'],\n",
       " [u'30', u' 65.44', u' 113.89'],\n",
       " [u'31', u' 69.52', u' 103.30'],\n",
       " [u'32', u' 65.81', u' 120.75'],\n",
       " [u'33', u' 67.82', u' 125.79'],\n",
       " [u'34', u' 70.60', u' 136.22'],\n",
       " [u'35', u' 71.80', u' 140.10'],\n",
       " [u'36', u' 69.21', u' 128.75'],\n",
       " [u'37', u' 66.80', u' 141.80'],\n",
       " [u'38', u' 67.66', u' 121.23'],\n",
       " [u'39', u' 67.81', u' 131.35'],\n",
       " [u'40', u' 64.05', u' 106.71'],\n",
       " [u'41', u' 68.57', u' 124.36'],\n",
       " [u'42', u' 65.18', u' 124.86'],\n",
       " [u'43', u' 69.66', u' 139.67'],\n",
       " [u'44', u' 67.97', u' 137.37'],\n",
       " [u'45', u' 65.98', u' 106.45'],\n",
       " [u'46', u' 68.67', u' 128.76'],\n",
       " [u'47', u' 66.88', u' 145.68'],\n",
       " [u'48', u' 67.70', u' 116.82'],\n",
       " [u'49', u' 69.82', u' 143.62'],\n",
       " [u'50', u' 69.09', u' 134.93'],\n",
       " [u'51', u' 69.91', u' 147.02'],\n",
       " [u'52', u' 67.33', u' 126.33'],\n",
       " [u'53', u' 70.27', u' 125.48'],\n",
       " [u'54', u' 69.10', u' 115.71'],\n",
       " [u'55', u' 65.38', u' 123.49'],\n",
       " [u'56', u' 70.18', u' 147.89'],\n",
       " [u'57', u' 70.41', u' 155.90'],\n",
       " [u'58', u' 66.54', u' 128.07'],\n",
       " [u'59', u' 66.36', u' 119.37'],\n",
       " [u'60', u' 67.54', u' 133.81'],\n",
       " [u'61', u' 66.50', u' 128.73'],\n",
       " [u'62', u' 69.00', u' 137.55'],\n",
       " [u'63', u' 68.30', u' 129.76'],\n",
       " [u'64', u' 67.01', u' 128.82'],\n",
       " [u'65', u' 70.81', u' 135.32'],\n",
       " [u'66', u' 68.22', u' 109.61'],\n",
       " [u'67', u' 69.06', u' 142.47'],\n",
       " [u'68', u' 67.73', u' 132.75'],\n",
       " [u'69', u' 67.22', u' 103.53'],\n",
       " [u'70', u' 67.37', u' 124.73'],\n",
       " [u'71', u' 65.27', u' 129.31'],\n",
       " [u'72', u' 70.84', u' 134.02'],\n",
       " [u'73', u' 69.92', u' 140.40'],\n",
       " [u'74', u' 64.29', u' 102.84'],\n",
       " [u'75', u' 68.25', u' 128.52'],\n",
       " [u'76', u' 66.36', u' 120.30'],\n",
       " [u'77', u' 68.36', u' 138.60'],\n",
       " [u'78', u' 65.48', u' 132.96'],\n",
       " [u'79', u' 69.72', u' 115.62'],\n",
       " [u'80', u' 67.73', u' 122.52'],\n",
       " [u'81', u' 68.64', u' 134.63'],\n",
       " [u'82', u' 66.78', u' 121.90'],\n",
       " [u'83', u' 70.05', u' 155.38'],\n",
       " [u'84', u' 66.28', u' 128.94'],\n",
       " [u'85', u' 69.20', u' 129.10'],\n",
       " [u'86', u' 69.13', u' 139.47'],\n",
       " [u'87', u' 67.36', u' 140.89'],\n",
       " [u'88', u' 70.09', u' 131.59'],\n",
       " [u'89', u' 70.18', u' 121.12'],\n",
       " [u'90', u' 68.23', u' 131.51'],\n",
       " [u'91', u' 68.13', u' 136.55'],\n",
       " [u'92', u' 70.24', u' 141.49'],\n",
       " [u'93', u' 71.49', u' 140.61'],\n",
       " [u'94', u' 69.20', u' 112.14'],\n",
       " [u'95', u' 70.06', u' 133.46'],\n",
       " [u'96', u' 70.56', u' 131.80'],\n",
       " [u'97', u' 66.29', u' 120.03'],\n",
       " [u'98', u' 63.43', u' 123.10'],\n",
       " [u'99', u' 66.77', u' 128.14'],\n",
       " [u'100', u' 68.89', u' 115.48'],\n",
       " [u'101', u' 64.87', u' 102.09'],\n",
       " [u'102', u' 67.09', u' 130.35'],\n",
       " [u'103', u' 68.35', u' 134.18'],\n",
       " [u'104', u' 65.61', u' 98.64'],\n",
       " [u'105', u' 67.76', u' 114.56'],\n",
       " [u'106', u' 68.02', u' 123.49'],\n",
       " [u'107', u' 67.66', u' 123.05'],\n",
       " [u'108', u' 66.31', u' 126.48'],\n",
       " [u'109', u' 69.44', u' 128.42'],\n",
       " [u'110', u' 63.84', u' 127.19'],\n",
       " [u'111', u' 67.72', u' 122.06'],\n",
       " [u'112', u' 70.05', u' 127.61'],\n",
       " [u'113', u' 70.19', u' 131.64'],\n",
       " [u'114', u' 65.95', u' 111.90'],\n",
       " [u'115', u' 70.01', u' 122.04'],\n",
       " [u'116', u' 68.61', u' 128.55'],\n",
       " [u'117', u' 68.81', u' 132.68'],\n",
       " [u'118', u' 69.76', u' 136.06'],\n",
       " [u'119', u' 65.46', u' 115.94'],\n",
       " [u'120', u' 68.83', u' 136.90'],\n",
       " [u'121', u' 65.80', u' 119.88'],\n",
       " [u'122', u' 67.21', u' 109.01'],\n",
       " [u'123', u' 69.42', u' 128.27'],\n",
       " [u'124', u' 68.94', u' 135.29'],\n",
       " [u'125', u' 67.94', u' 106.86'],\n",
       " [u'126', u' 65.63', u' 123.29'],\n",
       " [u'127', u' 66.50', u' 109.51'],\n",
       " [u'128', u' 67.93', u' 119.31'],\n",
       " [u'129', u' 68.89', u' 140.24'],\n",
       " [u'130', u' 70.24', u' 133.98'],\n",
       " [u'131', u' 68.27', u' 132.58'],\n",
       " [u'132', u' 71.23', u' 130.70'],\n",
       " [u'133', u' 69.10', u' 115.56'],\n",
       " [u'134', u' 64.40', u' 123.79'],\n",
       " [u'135', u' 71.10', u' 128.14'],\n",
       " [u'136', u' 68.22', u' 135.96'],\n",
       " [u'137', u' 65.92', u' 116.63'],\n",
       " [u'138', u' 67.44', u' 126.82'],\n",
       " [u'139', u' 73.90', u' 151.39'],\n",
       " [u'140', u' 69.98', u' 130.40'],\n",
       " [u'141', u' 69.52', u' 136.21'],\n",
       " [u'142', u' 65.18', u' 113.40'],\n",
       " [u'143', u' 68.01', u' 125.33'],\n",
       " [u'144', u' 68.34', u' 127.58'],\n",
       " [u'145', u' 65.18', u' 107.16'],\n",
       " [u'146', u' 68.26', u' 116.46'],\n",
       " [u'147', u' 68.57', u' 133.84'],\n",
       " [u'148', u' 64.50', u' 112.89'],\n",
       " [u'149', u' 68.71', u' 130.76'],\n",
       " [u'150', u' 68.89', u' 137.76'],\n",
       " [u'151', u' 69.54', u' 125.40'],\n",
       " [u'152', u' 67.40', u' 138.47'],\n",
       " [u'153', u' 66.48', u' 120.82'],\n",
       " [u'154', u' 66.01', u' 140.15'],\n",
       " [u'155', u' 72.44', u' 136.74'],\n",
       " [u'156', u' 64.13', u' 106.11'],\n",
       " [u'157', u' 70.98', u' 158.96'],\n",
       " [u'158', u' 67.50', u' 108.79'],\n",
       " [u'159', u' 72.02', u' 138.78'],\n",
       " [u'160', u' 65.31', u' 115.91'],\n",
       " [u'161', u' 67.08', u' 146.29'],\n",
       " [u'162', u' 64.39', u' 109.88'],\n",
       " [u'163', u' 69.37', u' 139.05'],\n",
       " [u'164', u' 68.38', u' 119.90'],\n",
       " [u'165', u' 65.31', u' 128.31'],\n",
       " [u'166', u' 67.14', u' 127.24'],\n",
       " [u'167', u' 68.39', u' 115.23'],\n",
       " [u'168', u' 66.29', u' 124.80'],\n",
       " [u'169', u' 67.19', u' 126.95'],\n",
       " [u'170', u' 65.99', u' 111.27'],\n",
       " [u'171', u' 69.43', u' 122.61'],\n",
       " [u'172', u' 67.97', u' 124.21'],\n",
       " [u'173', u' 67.76', u' 124.65'],\n",
       " [u'174', u' 65.28', u' 119.52'],\n",
       " [u'175', u' 73.83', u' 139.30'],\n",
       " [u'176', u' 66.81', u' 104.83'],\n",
       " [u'177', u' 66.89', u' 123.04'],\n",
       " [u'178', u' 65.74', u' 118.89'],\n",
       " [u'179', u' 65.98', u' 121.49'],\n",
       " [u'180', u' 66.58', u' 119.25'],\n",
       " [u'181', u' 67.11', u' 135.02'],\n",
       " [u'182', u' 65.87', u' 116.23'],\n",
       " [u'183', u' 66.78', u' 109.17'],\n",
       " [u'184', u' 68.74', u' 124.22'],\n",
       " [u'185', u' 66.23', u' 141.16'],\n",
       " [u'186', u' 65.96', u' 129.15'],\n",
       " [u'187', u' 68.58', u' 127.87'],\n",
       " [u'188', u' 66.59', u' 120.92'],\n",
       " [u'189', u' 66.97', u' 127.65'],\n",
       " [u'190', u' 68.08', u' 101.47'],\n",
       " [u'191', u' 70.19', u' 144.99'],\n",
       " [u'192', u' 65.52', u' 110.95'],\n",
       " [u'193', u' 67.46', u' 132.86'],\n",
       " [u'194', u' 67.41', u' 146.34'],\n",
       " [u'195', u' 69.66', u' 145.59'],\n",
       " [u'196', u' 65.80', u' 120.84'],\n",
       " [u'197', u' 66.11', u' 115.78'],\n",
       " [u'198', u' 68.24', u' 128.30'],\n",
       " [u'199', u' 68.02', u' 127.47'],\n",
       " [u'200', u' 71.39', u' 127.88 ']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PipelinedRDD' object has no attribute 'toDf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-c94db34d022d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PipelinedRDD' object has no attribute 'toDf'"
     ]
    }
   ],
   "source": [
    "df = Rdd.toDf()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `get_text` not found.\n"
     ]
    }
   ],
   "source": [
    "get_text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.text(os.path.join(\"data\", \"ds_spark_heightweight.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+\n",
      "|value                                     |\n",
      "+------------------------------------------+\n",
      "|\"Index\", Height(Inches)\", \"Weight(Pounds)\"|\n",
      "|1, 65.78, 112.99                          |\n",
      "|2, 71.52, 136.49                          |\n",
      "|3, 69.40, 153.03                          |\n",
      "|4, 68.22, 142.34                          |\n",
      "|5, 67.79, 144.30                          |\n",
      "|6, 68.70, 123.30                          |\n",
      "|7, 69.80, 141.49                          |\n",
      "|8, 70.01, 136.46                          |\n",
      "|9, 67.90, 112.37                          |\n",
      "|10, 66.78, 120.67                         |\n",
      "|11, 66.49, 127.45                         |\n",
      "|12, 67.62, 114.14                         |\n",
      "|13, 68.30, 125.61                         |\n",
      "|14, 67.12, 122.46                         |\n",
      "|15, 68.28, 116.09                         |\n",
      "|16, 71.09, 140.00                         |\n",
      "|17, 66.46, 129.50                         |\n",
      "|18, 68.65, 142.97                         |\n",
      "|19, 71.23, 137.90                         |\n",
      "+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Index\", Height(Inches)\", \"Weight(Pounds)\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1, 65.78, 112.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2, 71.52, 136.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3, 69.40, 153.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4, 68.22, 142.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        value\n",
       "0  \"Index\", Height(Inches)\", \"Weight(Pounds)\"\n",
       "1                            1, 65.78, 112.99\n",
       "2                            2, 71.52, 136.49\n",
       "3                            3, 69.40, 153.03\n",
       "4                            4, 68.22, 142.34"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-ab57f1b3871f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\user\\spark-2.0.0-bin-hadoop2.7\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    842\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             raise AttributeError(\n\u001b[1;32m--> 844\u001b[1;33m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[0;32m    845\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'data'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
