# 스파크설치
* https://databricks.com/ : 스파크 설치안되면 여기 들어가서 주피터노트북으로 사용가능

## 1. spark 설치
- [https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html) : 스파크 설치 사이트

- 사이트에서 'Spark release archives' 클릭 후 **'tar -xvzf spark-2.0.0-bin-hadoop2.7.tgz'** 설치
	![image](https://user-images.githubusercontent.com/47033052/66460083-2c9b5880-eab1-11e9-924c-f9e910a2e378.png)  

## 2. 자바 설치
- 자바는 Java SE Development Kit 8u221 
- 링크 : [https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html](https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)

## 3. 파이썬 설치
- 파이썬 버전은 2.7.x 버전 권장
	- 나는 일단은 3.6.7 존버
	- 사실 존버 실패해서 2.7로 다운그레이드함.

## 4. 'winutils.exe' 설치
- 윈도우에서 하둡을 따로 설치한다면 패스해도 되는 과정.
- [https://github.com/steveloughran/winutils/](https://github.com/steveloughran/winutils/)에서 'winutils.exe' 다운로드
	- C:\hadoop\bin\에 winutils.exe 넣기. 

## 5. 경로 설정(윈도우)
- SPARK_HOME : "C:\Users\user\spark-2.0.0-bin-hadoop2.7"
- JAVA_HOME : "C:\Program Files\Java\jdk1.8.0_221"
- HADOOP_HOME : "C:\hadoop"
- Path : %SPARK_HOME%\bin;%JAVA_HOME%\bin;%HADOOP_HOME%\bin;"

## 6. CMD에서 경로 설정이 올바른지 확인
![image](https://user-images.githubusercontent.com/47033052/66466417-a20d2600-eabd-11e9-8866-fd3d862766a3.png)

## 7. 스파크 실행
### cmd에서 실행하기
- 스파크디렉토리\bin 에서 cmd 실행.
	- ![image](https://user-images.githubusercontent.com/47033052/66539635-aee45500-eb63-11e9-9a29-4d9dd9cef533.png)

- cmd에서 spark-shell 실행하기
	- ![image](https://user-images.githubusercontent.com/47033052/66539690-e3f0a780-eb63-11e9-9b44-25250288be5c.png)

- python 쓸 겨우 pyspark 실행

### Jupyter Notebook에서 실행하기
(**선호하는 방법**)

- 1. Jupyter Notebook 실행
- 2. SPARK_HOME 및 PYTHONPATH를 설정
```
import os
import sys

#home=os.path.expanduser("~") # HOME이 설정되어 있지 않으면 expanduser('~')를 사용한다.
#osn.environ["PYSPARK_PYTHON"] = "/usr/bin/python"
os.environ["SPARK_HOME"]=os.path.join(os.path.expanduser("~"),r"C:\Users\user\spark-2.0.0-bin-hadoop2.7\spark-2.0.0-bin-hadoop2.7")
os.environ["PYLIB"]=os.path.join(os.environ["SPARK_HOME"],'python','lib')
sys.path.insert(0,os.path.join(os.environ["PYLIB"],'py4j-0.10.1-src.zip'))
sys.path.insert(0,os.path.join(os.environ["PYLIB"],'pyspark.zip'))
```
- 3. SparkSession을 생성
```
import pyspark
myConf=pyspark.SparkConf()
spark = pyspark.sql.SparkSession.builder\
    .master("local")\
    .appName("myApp")\
    .config(conf=myConf)\
    .getOrCreate()

```

![image](https://user-images.githubusercontent.com/47033052/66549766-fd531d00-eb7e-11e9-9880-610e89948ea3.png)

